{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7d23f-57eb-4b38-a9f5-9ecc1aec6eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d9075c-82c1-4ac4-b3c4-573f5ca190c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€” imports & reproducibility\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "# reproducibility (best-effort)\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80479412-9ea3-41b3-b7d9-cd12b657ff0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal files: 39\n",
      "Schizo files: 45\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” adjust these paths to your dataset folders\n",
    "normal_path = r\"C:\\Users\\HP\\Downloads\\norm\"        # files ending with .eea\n",
    "schizo_path = r\"C:\\Users\\HP\\Downloads\\sch\"\n",
    "\n",
    "# quick print\n",
    "print(\"Normal files:\", len([f for f in os.listdir(normal_path) if f.endswith('.eea')]))\n",
    "print(\"Schizo files:\", len([f for f in os.listdir(schizo_path) if f.endswith('.eea')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7e6846a-20c4-4049-84e8-15a6d81ef98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal lengths summary: 122880 122880.0 122880\n",
      "Schizophrenic lengths summary: 122880 122880.0 122880\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” load file lengths to decide windowing/fixed length\n",
    "def read_signal(filepath):\n",
    "    return np.loadtxt(filepath)   # your files are plain numeric text\n",
    "\n",
    "def get_lengths(folder):\n",
    "    lengths = []\n",
    "    files = [f for f in os.listdir(folder) if f.endswith('.eea')]\n",
    "    for f in files:\n",
    "        s = read_signal(os.path.join(folder, f))\n",
    "        lengths.append(len(s))\n",
    "    return np.array(lengths), files\n",
    "\n",
    "lens_norm, files_norm = get_lengths(normal_path)\n",
    "lens_scz, files_scz = get_lengths(schizo_path)\n",
    "\n",
    "print(\"Normal lengths summary:\", np.min(lens_norm), np.median(lens_norm), np.max(lens_norm))\n",
    "print(\"Schizophrenic lengths summary:\", np.min(lens_scz), np.median(lens_scz), np.max(lens_scz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a7ab0a5-5e03-4d5a-bac4-7676721c11ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (84, 500, 1) y shape: (84,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4A â€” parameters\n",
    "FIXED_LEN = 500  # choose (e.g., 500). You can also set FIXED_LEN = min(all_lengths) after inspecting.\n",
    "\n",
    "def load_eeg_fixed(folder_path, label, fixed_len=FIXED_LEN):\n",
    "    X, y, ids = [], [], []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.endswith('.eea'):\n",
    "            continue\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        sig = read_signal(path).astype(np.float32)\n",
    "        # normalize per-signal (z-score)\n",
    "        sig = (sig - sig.mean()) / (sig.std() + 1e-8)\n",
    "        # pad/trim\n",
    "        if len(sig) > fixed_len:\n",
    "            sig = sig[:fixed_len]\n",
    "        elif len(sig) < fixed_len:\n",
    "            sig = np.pad(sig, (0, fixed_len - len(sig)), 'constant')\n",
    "        X.append(sig)\n",
    "        y.append(label)\n",
    "        ids.append(fname)  # file/subject id\n",
    "    return np.array(X), np.array(y), np.array(ids)\n",
    "\n",
    "Xn, yn, idn = load_eeg_fixed(normal_path, 0)\n",
    "Xs, ys, ids = load_eeg_fixed(schizo_path, 1)\n",
    "X = np.vstack([Xn, Xs])\n",
    "y = np.hstack([yn, ys])\n",
    "ids_all = np.hstack([idn, ids])\n",
    "\n",
    "# reshape for keras: (samples, timesteps, channels)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bce489-97a4-4977-bb42-2eec6c781a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windowed X shape: (161028, 256, 1) y shape: (161028,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4B â€” windowing parameters\n",
    "WINDOW = 256        # window length in samples\n",
    "STRIDE = 64         # step between windows (overlap = WINDOW - STRIDE)\n",
    "\n",
    "def windows_from_file(sig, window=WINDOW, stride=STRIDE):\n",
    "    segs = []\n",
    "    for start in range(0, len(sig) - window + 1, stride):\n",
    "        seg = sig[start:start + window]\n",
    "        segs.append(seg)\n",
    "    return np.array(segs)  # shape = (num_windows, window)\n",
    "\n",
    "def load_eeg_windows(folder_path, label, window=WINDOW, stride=STRIDE):\n",
    "    X_list, y_list, subj_list = [], [], []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.endswith('.eea'):\n",
    "            continue\n",
    "        path = os.path.join(folder_path, fname)\n",
    "        sig = read_signal(path).astype(np.float32)\n",
    "        sig = (sig - sig.mean()) / (sig.std() + 1e-8)\n",
    "        segs = windows_from_file(sig, window, stride)\n",
    "        if len(segs) == 0:\n",
    "            # If signal shorter than window, pad to window once\n",
    "            padded = np.pad(sig, (0, max(0, window - len(sig))), 'constant')[:window]\n",
    "            segs = np.expand_dims(padded, axis=0)\n",
    "        X_list.append(segs)\n",
    "        y_list.append(np.ones(segs.shape[0], dtype=int) * label)\n",
    "        subj_list.extend([fname]*segs.shape[0])\n",
    "    X = np.vstack(X_list)\n",
    "    y = np.hstack(y_list)\n",
    "    subj_ids = np.array(subj_list)\n",
    "    return X, y, subj_ids\n",
    "\n",
    "# load windows for both classes\n",
    "Xn_w, yn_w, idn_w = load_eeg_windows(normal_path, 0)\n",
    "Xs_w, ys_w, ids_w = load_eeg_windows(schizo_path, 1)\n",
    "X_w = np.vstack([Xn_w, Xs_w])         # (N_samples, WINDOW)\n",
    "y_w = np.hstack([yn_w, ys_w])\n",
    "ids_w = np.hstack([idn_w, ids_w])\n",
    "\n",
    "# reshape for keras\n",
    "X_w = X_w.reshape((X_w.shape[0], X_w.shape[1], 1))\n",
    "print(\"Windowed X shape:\", X_w.shape, \"y shape:\", y_w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a0a6cd-2327-43d9-bc87-6c908efa3217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€” model builders\n",
    "def build_cnn(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling1D(2),\n",
    "        layers.Conv1D(256, kernel_size=3, activation='relu'),\n",
    "        layers.GlobalAveragePooling1D(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=input_shape),\n",
    "        layers.Bidirectional(layers.LSTM(32)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0842fd-9751-409c-8070-3898fd1b0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 â€” callbacks\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "# model checkpoint will be added inside training loop per-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26da14e7-f468-4411-9d6e-d94deee4ba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (128822, 256, 1) Test: (32206, 256, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from None to 0.35301, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 156s - 12ms/step - accuracy: 0.7445 - loss: 0.4986 - val_accuracy: 0.8501 - val_loss: 0.3530 - learning_rate: 1.0000e-03\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.35301 to 0.28868, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 147s - 11ms/step - accuracy: 0.8533 - loss: 0.3482 - val_accuracy: 0.8819 - val_loss: 0.2887 - learning_rate: 1.0000e-03\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.28868 to 0.25796, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 446s - 35ms/step - accuracy: 0.8757 - loss: 0.3010 - val_accuracy: 0.8999 - val_loss: 0.2580 - learning_rate: 1.0000e-03\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.25796 to 0.24496, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 147s - 11ms/step - accuracy: 0.8924 - loss: 0.2636 - val_accuracy: 0.9043 - val_loss: 0.2450 - learning_rate: 1.0000e-03\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.24496\n",
      "12883/12883 - 258s - 20ms/step - accuracy: 0.9056 - loss: 0.2295 - val_accuracy: 0.9052 - val_loss: 0.2584 - learning_rate: 1.0000e-03\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.24496 to 0.20827, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 146s - 11ms/step - accuracy: 0.9195 - loss: 0.2013 - val_accuracy: 0.9223 - val_loss: 0.2083 - learning_rate: 1.0000e-03\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.20827\n",
      "12883/12883 - 146s - 11ms/step - accuracy: 0.9305 - loss: 0.1781 - val_accuracy: 0.9138 - val_loss: 0.2319 - learning_rate: 1.0000e-03\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.20827 to 0.18647, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 713s - 55ms/step - accuracy: 0.9406 - loss: 0.1544 - val_accuracy: 0.9300 - val_loss: 0.1865 - learning_rate: 1.0000e-03\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.18647\n",
      "12883/12883 - 147s - 11ms/step - accuracy: 0.9479 - loss: 0.1365 - val_accuracy: 0.9224 - val_loss: 0.2132 - learning_rate: 1.0000e-03\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.18647\n",
      "12883/12883 - 147s - 11ms/step - accuracy: 0.9531 - loss: 0.1243 - val_accuracy: 0.9346 - val_loss: 0.1989 - learning_rate: 1.0000e-03\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.18647 to 0.15400, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 520s - 40ms/step - accuracy: 0.9578 - loss: 0.1123 - val_accuracy: 0.9448 - val_loss: 0.1540 - learning_rate: 1.0000e-03\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.15400\n",
      "12883/12883 - 147s - 11ms/step - accuracy: 0.9621 - loss: 0.1038 - val_accuracy: 0.9346 - val_loss: 0.1947 - learning_rate: 1.0000e-03\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.15400\n",
      "12883/12883 - 153s - 12ms/step - accuracy: 0.9659 - loss: 0.0962 - val_accuracy: 0.9356 - val_loss: 0.1980 - learning_rate: 1.0000e-03\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.15400\n",
      "12883/12883 - 147s - 11ms/step - accuracy: 0.9678 - loss: 0.0889 - val_accuracy: 0.9308 - val_loss: 0.2189 - learning_rate: 1.0000e-03\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss improved from 0.15400 to 0.15349, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 331s - 26ms/step - accuracy: 0.9711 - loss: 0.0810 - val_accuracy: 0.9520 - val_loss: 0.1535 - learning_rate: 1.0000e-03\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.15349\n",
      "12883/12883 - 149s - 12ms/step - accuracy: 0.9732 - loss: 0.0777 - val_accuracy: 0.9298 - val_loss: 0.2395 - learning_rate: 1.0000e-03\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss improved from 0.15349 to 0.14256, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 533s - 41ms/step - accuracy: 0.9747 - loss: 0.0718 - val_accuracy: 0.9532 - val_loss: 0.1426 - learning_rate: 1.0000e-03\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.14256\n",
      "12883/12883 - 149s - 12ms/step - accuracy: 0.9763 - loss: 0.0695 - val_accuracy: 0.9551 - val_loss: 0.1671 - learning_rate: 1.0000e-03\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.14256\n",
      "12883/12883 - 147s - 11ms/step - accuracy: 0.9780 - loss: 0.0646 - val_accuracy: 0.9550 - val_loss: 0.1611 - learning_rate: 1.0000e-03\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.14256\n",
      "12883/12883 - 151s - 12ms/step - accuracy: 0.9793 - loss: 0.0627 - val_accuracy: 0.9577 - val_loss: 0.1733 - learning_rate: 1.0000e-03\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.14256\n",
      "12883/12883 - 150s - 12ms/step - accuracy: 0.9808 - loss: 0.0586 - val_accuracy: 0.9555 - val_loss: 0.1632 - learning_rate: 1.0000e-03\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.14256\n",
      "12883/12883 - 151s - 12ms/step - accuracy: 0.9807 - loss: 0.0578 - val_accuracy: 0.9499 - val_loss: 0.2494 - learning_rate: 1.0000e-03\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.14256\n",
      "12883/12883 - 161s - 12ms/step - accuracy: 0.9909 - loss: 0.0288 - val_accuracy: 0.9612 - val_loss: 0.2077 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.14256\n",
      "12883/12883 - 98s - 8ms/step - accuracy: 0.9926 - loss: 0.0240 - val_accuracy: 0.9670 - val_loss: 0.1590 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.14256\n",
      "12883/12883 - 77s - 6ms/step - accuracy: 0.9932 - loss: 0.0218 - val_accuracy: 0.9641 - val_loss: 0.2159 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss improved from 0.14256 to 0.13873, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12883/12883 - 77s - 6ms/step - accuracy: 0.9934 - loss: 0.0222 - val_accuracy: 0.9698 - val_loss: 0.1387 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.13873\n",
      "12883/12883 - 78s - 6ms/step - accuracy: 0.9940 - loss: 0.0208 - val_accuracy: 0.9686 - val_loss: 0.1477 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.13873\n",
      "12883/12883 - 81s - 6ms/step - accuracy: 0.9941 - loss: 0.0196 - val_accuracy: 0.9672 - val_loss: 0.1861 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.13873\n",
      "12883/12883 - 76s - 6ms/step - accuracy: 0.9942 - loss: 0.0196 - val_accuracy: 0.9689 - val_loss: 0.1501 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.13873\n",
      "12883/12883 - 76s - 6ms/step - accuracy: 0.9945 - loss: 0.0178 - val_accuracy: 0.9599 - val_loss: 0.2421 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.13873\n",
      "12883/12883 - 76s - 6ms/step - accuracy: 0.9946 - loss: 0.0186 - val_accuracy: 0.9680 - val_loss: 0.1689 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.13873\n",
      "12883/12883 - 75s - 6ms/step - accuracy: 0.9972 - loss: 0.0088 - val_accuracy: 0.9742 - val_loss: 0.1546 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.13873\n",
      "12883/12883 - 85s - 7ms/step - accuracy: 0.9981 - loss: 0.0070 - val_accuracy: 0.9718 - val_loss: 0.2067 - learning_rate: 2.5000e-04\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.13873\n",
      "12883/12883 - 86s - 7ms/step - accuracy: 0.9981 - loss: 0.0071 - val_accuracy: 0.9716 - val_loss: 0.2033 - learning_rate: 2.5000e-04\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.13873\n",
      "12883/12883 - 80s - 6ms/step - accuracy: 0.9982 - loss: 0.0062 - val_accuracy: 0.9728 - val_loss: 0.1877 - learning_rate: 2.5000e-04\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.13873\n",
      "12883/12883 - 78s - 6ms/step - accuracy: 0.9983 - loss: 0.0065 - val_accuracy: 0.9725 - val_loss: 0.2085 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” choose data source\n",
    "USE_WINDOWS = True\n",
    "\n",
    "if USE_WINDOWS:\n",
    "    X_data, y_data = X_w, y_w\n",
    "else:\n",
    "    X_data, y_data = X, y\n",
    "\n",
    "# stratified split (note: if using windows, windows from same subject may leak -> do group-split instead)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, stratify=y_data, random_state=seed\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# build model and train\n",
    "input_shape = X_train.shape[1:]\n",
    "model = build_cnn(input_shape)   # or build_lstm(input_shape)\n",
    "\n",
    "checkpoint_path = \"best_model.h5\"\n",
    "mc = callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    callbacks=[es, reduce_lr, mc],\n",
    "    verbose=2\n",
    "),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "012c04ac-2b02-4d93-88c6-656a5dbf50d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.13873\n",
      "12883/12883 - 150s - 12ms/step - accuracy: 0.9973 - loss: 0.0094 - val_accuracy: 0.9719 - val_loss: 0.1570 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.13873\n",
      "12883/12883 - 148s - 11ms/step - accuracy: 0.9984 - loss: 0.0060 - val_accuracy: 0.9734 - val_loss: 0.1622 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.13873\n",
      "12883/12883 - 205s - 16ms/step - accuracy: 0.9986 - loss: 0.0054 - val_accuracy: 0.9726 - val_loss: 0.1578 - learning_rate: 1.2500e-04\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.13873\n",
      "12883/12883 - 150s - 12ms/step - accuracy: 0.9988 - loss: 0.0049 - val_accuracy: 0.9753 - val_loss: 0.1537 - learning_rate: 1.2500e-04\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.13873\n",
      "12883/12883 - 157s - 12ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 0.9734 - val_loss: 0.1721 - learning_rate: 1.2500e-04\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.13873\n",
      "12883/12883 - 155s - 12ms/step - accuracy: 0.9992 - loss: 0.0033 - val_accuracy: 0.9754 - val_loss: 0.1626 - learning_rate: 6.2500e-05\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.13873\n",
      "12883/12883 - 148s - 11ms/step - accuracy: 0.9994 - loss: 0.0026 - val_accuracy: 0.9739 - val_loss: 0.1855 - learning_rate: 6.2500e-05\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.13873\n",
      "12883/12883 - 151s - 12ms/step - accuracy: 0.9995 - loss: 0.0023 - val_accuracy: 0.9757 - val_loss: 0.1667 - learning_rate: 6.2500e-05\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.13873\n",
      "12883/12883 - 154s - 12ms/step - accuracy: 0.9994 - loss: 0.0031 - val_accuracy: 0.9769 - val_loss: 0.1559 - learning_rate: 6.2500e-05\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.13873\n",
      "12883/12883 - 153s - 12ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 0.9755 - val_loss: 0.1680 - learning_rate: 6.2500e-05\n"
     ]
    }
   ],
   "source": [
    "# Continue training for more epochs\n",
    "history2 = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,          # more epochs\n",
    "    initial_epoch=len(history.epoch),  # resume from previous epoch\n",
    "    batch_size=8,\n",
    "    callbacks=[es, reduce_lr, mc],\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7889c69-cc85-4b17-b50e-f3ffc982e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1007/1007\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9747 - loss: 0.1383\n",
      "Test Accuracy: 0.9747, Test Loss: 0.1383\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169af75-45b4-43ae-bf8c-38d13495df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save(\"schizophrenia_detection_model_final.h5\")\n",
    "\n",
    "# Load it anytime later\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"schizophrenia_detection_model_final.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8422c8f-4ddb-4a61-8e48-06bb4f8a461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample EEG file saved at: C:/Users/HP/Downloads/new_sample.eea\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate random EEG-like data (you can adjust size to match your training, e.g., 256 samples)\n",
    "sample_eeg = np.random.normal(0, 100, 256)  # mean=0, std=100 to mimic EEG voltage variations\n",
    "\n",
    "# Save as .eea file\n",
    "np.savetxt(\"C:/Users/HP/Downloads/new_sample.eea\", sample_eeg, fmt=\"%.2f\")\n",
    "\n",
    "print(\"âœ… Sample EEG file saved at: C:/Users/HP/Downloads/new_sample.eea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04e38246-5c78-4888-b0d9-5acff89e6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
      "ðŸ™‚ Normal (Healthy)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example: load one new EEG sample\n",
    "new_data = np.loadtxt(\"C:/Users/HP/Downloads/new_sample.eea\")\n",
    "new_data = new_data.reshape(1, 256, 1)  # reshape like your training data\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(new_data)\n",
    "\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"ðŸ§  Schizophrenia Detected\")\n",
    "else:\n",
    "    print(\"ðŸ™‚ Normal (Healthy)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b85cd2-b02b-4a05-9a4e-094ae8f97a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\jupyter notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e88121-ee21-4d4e-8ee1-c29c4c446aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
